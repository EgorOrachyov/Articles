\section{Results}

The following results were achieved in this work.

\begin{itemize}
    \item \textbf{The survey of the field was conducted}. Model for a graph analysis were shown. Also, the concept of the linear algebra based approach was described in a great detail with a respect to a graph traversal and existing solutions. Introduction into a GraphBLAS standard was provided. Existing implementations, frameworks and most significant contributions for a graph analytic were studied. Their limitations were highlighted. General-purpose GPU computations concept was covered. Different APIs for GPU programming were presented. Their advantages and disadvantages were covered. General GPU programming challenges and pitfalls were highlighted. 
    
    \item \textbf{The architecture of the library for a generalized sparse linear algebra for GPU computations was developed}. The architecture and library design was based on a project requirements, as well as on a limitation and experience of the existing solutions. The differences with GraphBLAS standard were stated.
    
    \item \textbf{The library was implemented accordingly to the developed architecture}. The core of the library, interface, data containers, built-in scalar data types, built-in element-wise functions, expressions processing, OpenCL backend functionality, common linear algebra operations implementations were covered. Several graph algorithms for a graph analysis were implemented using developed library API.
    
    \item \textbf{The preliminary experimental study of the proposed artifacts was conducted}. Obtained results allowed to conclude, that the chosen method of the library development is a promising way to a high-performance graph analysis in terms of the linear algebra on a wide family of GPU devices. The proposed solution showed comparable performance to the GraphBLAST, reaching up to $\times 36$ speedup in some cases, and showed constantly better performance, than SuiteSparse, reaching up to $\times 20$ speedup in some cases. The developed library showed a scalability on different device vendors GPUs. Also, the proposed solutions got an acceptable performance on integrated GPUs in some cases even if compared with highly-optimized multi-core CPU frameworks.
\end{itemize}

All in all, there are still a plenty of research questions and directions for improvement. Some of them are listed bellow.

\begin{itemize}
    \item \textbf{Performance tuning}. There is a still space for optimizations. Better workload balancing must be done. Performance must be improved on AMD and Intel devices. More optimized algorithms must be implemented, such as SpGEMM  algorithm proposed by Nagasaka et al.~\cite{inproceedings:spgemm_mem_saving_for_nvidia} for general \textit{mxm} operation.
   
    \item \textbf{Operations}. Additional linear algebra operations must be implemented as well as useful subroutines for filtering, joining, loading, saving data, and other manipulations involved in typical graphs analysis.
    
    \item \textbf{Graph streaming}. The next important direction of the study is streaming of data from CPU to GPU for out-of-memory graphs analysis. CuSha adopt data partitioning techniques for graphs processing which do not fit single GPU. Modern GPUs have a limited VRAM. Even high-end devices allow only a moderate portion of the memory to be addressed by the kernel at the same time. Thus, manual streaming of the data from CPU to GPU is required in order to support analysis of extremely large graphs, which count billions of edges to process.
    
    \item \textbf{Multi-GPU}. Finally, scaling of the library to multiple GPUs must be implemented. Gunrock shows, that such approach can increase overall throughput and speedup processing of really dense graph. In connection with a streaming, it can be an ultimate solution for a large real-world graphs analysis.
\end{itemize}

The library source code is published on a GitHub platform. It is available at \url{https://github.com/SparseLinearAlgebra/spla}.